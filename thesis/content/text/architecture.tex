% -*- root: ../../main.tex -*- %

\subsection{Choice of an architecture style} % (fold)
  \label{sub:choice_of_an_achitecture_model}
  One central requirement for the stated objectives is the modularization of the application. It enables the containment of failures, the replacement or upgrading of components at runtime, and the individual scaling of parts of the \ac{WfMS}. In \ref{cha:architecture_styles}, three architecture styles are introduced. The concept of \ac{SOA} and \ac{MSA} inherently requires the modularization of code, while it is optional -- yet advisable -- in a monolithic architecture.

  While measures can be taken in monolithic applications to limit the effect of component failures, the whole application is rendered inoperative if the underlying machine fails or the process dies \cite[p.~55]{Newman2015Building}. \ac{SOA} and \ac{MSA} both strongly advocate to account for the possibility of a non-responding service -- in the first place because of the limited reliability of network communication, but the outcome applies to any other reason for failure, too. They hence inherently support the objective of resilience better than monolithic architecture.

  Upgrading or replacing components of an application at runtime is possible in each of the presented architectures. In \ac{SOA} the service may be replaced at will by directing requests to an instance of the new version of that service, given that the previously exhibited behavior does not change. The same applies for \ac{MSA}, but as there is no direct messaging, the replaced components only have to adhere to the expected messaging scheme. Monolithic applications may introduce patterns such as dependency injection and dynamic loading to make changes at runtime possible.

  \ac{SOA} and \ac{MSA} both permit scaling individual parts of an application by duplicating services.
  With a monolithic architecture, scaling the whole application is usually easier than in \ac{SOA} and \ac{MSA} -- in most cases, another instance of the application may be started for that purpose -- but it is not possible to scale only those parts of an application where performance bottlenecks arise.

  In regards of the above considerations, monolithic architecture is ruled out as the favored application structure. Thus, only \ac{SOA} and \ac{MSA} are left for the decision regarding the \ac{WfMS}' architecture of choice.

  The Docker Ecosystem facilitates the setup of the infrastructure for a \ac{MSA}. As stated in \ref{par:micro_services_architecture}, the \ac{MOM} itself contains little to no knowledge about the system using it. Thus, the \ac{MOM} and all application components may simply be started in separate containers and connected using an overlay network. Docker further permits the configuration of restart policies for specific containers. In case that one container crashes, it is restarted with its previous settings, if configured so.

  This reasoning leads to the overall conclusion, that \ac{MSA} is the architecture of choice for the prototype with regards to the chosen objectives.
  % subsection choice_of_an_achitecture_model (end)

\subsection{User interaction with the system} % (fold)
\label{sub:user_interaction_with_the_system}
  In a monolithic architecture, the use of a single user interface that provides access to the whole functionality of the \ac{WfMS} would appear to be the obvious choice.
  In contrast to that, the modular structure of a \ac{MSA} with clearly defined borders intuitively promotes separate user interfaces for different functionalities.
  In a \ac{MSA}, either each service offers its own user interface or there is a component at some point between the user and the \ac{WfMS} that consolidates the interaction. As this architectural style is chosen in \ref{sub:choice_of_an_achitecture_model}, the advantages and disadvantages of both options are briefly discussed in the following.

  Separate user interfaces increase the flexibility regarding changes in both frontend and backend of a service. While the adaption of a consolidating component to a change within one service would likely require its redeployment and thus affect the availability of other services, this is not the case if a dedicated user interface is in place, which can be restarted without affecting other services.
  A disadvantage of separate access to the different services is, that the user needs to know the location of each service in order to address it. Another drawback is that in this setting, tasks such as authentication, load balancing etc. would have to be performed for each service separately. Also, if the \ac{API} of a service changes, all clients have to be adapted.

  One way to provide a unified access to the system is the introduction of an \ac{API} gateway.
  This component can forward incoming requests to the appropriate services or send multiple concurrent requests and return their aggregated responses. This reduces the amount of requests the client has to send to the system. \ac{API} gateways also eliminate the need of client software to incorporate knowledge on the \ac{WfMS}' internal structure, \eg which services exist and how to contact them.
  A downside of the \ac{API} gateway pattern is, that it introduces a new single point of failure to the system which has to be countered, \eg by replicating the gateway and let a proxy server route requests to healthy instances. Another downside is that the gateway represents another independent service that has to be maintained.

  \ac{API} gateways are used for this prototype, as they allow to de-duplicate the logic required to transform client requests to messages to the \ac{MOM}, since this is the way of communication chosen in \ref{sub:application_level_communication}. Further, they minimize the complexity of the prototype's services, as separate user interfaces for the services would require each service to provide server functionality with routing configuration, view rendering, etc.
% subsection user_interaction_with_the_system (end)
\subsection{Inter-component communication} % (fold)
\label{sub:inter_component_communication}

  Since all services reside in separate containers, \ie are isolated through namespaces for processes, networks etc, a means must be found to let them communicate with each other.

  The na√Øve approach would be to let each container expose its required ports on the host's network interface. In order to communicate with a service in another container, an application would then contact the host machine \ac{IP} address on the respective port. While this solution appeals because of its simplicity, it comes with considerable drawbacks.
  First, a port can only be used by one application at a time. This poses a problem as soon as a container is run more than once simultaneously, \eg if multiple services require an instance of the same database application or a service is started several times for scaling.
  Second, this exposes the services in question to requests from any computer that can communicate with the host machine. Unless this is desired behavior, it creates an unnecessary attack surface.

  Another approach is the use of a Docker feature called \emph{links}, which allows to specify direct connections between containers based on their names.
  Docker creates a secure tunnel between the specified containers and provides information on how the link source container may be addressed to the recipient container. This happens at two places: it passes along all environment variables of the link source container to the targeted container and updates the \texttt{/etc/hosts} file, which is responsible for manual resolution of hostnames to \ac{IP} addresses.
  Links can be specified when a container is created by referring to one or more already running containers.
  Linked containers can be contacted via their hostname from within the started container \cite{Docker2016Docker}.

  While links -- in contrast to the first approach -- allow the same port to be used in different containers and do not expose the containers, they have disadvantages, too.
  First, and most important, they are static \cite{Docker2016Docker}. That is, restarting one container breaks the link functionality. This is problematic, as the re-deployment of a service that relies on links or is linked to may require a domino-like chain of container restarts to restore the linking behavior.
  Second, they do only work on the same host \cite{Docker2016Docker}. This solution thus does not support the distributed execution of the \ac{WfMS} micro-services, unless directly related containers, \eg a service and its database, are placed on the same node and all other communication takes place via exposed ports on the respective host machines.
  Third, \emph{all} environment variables that Docker created within a container are passed to any container that links to it, which could pose problems regarding security if they contain sensitive data such as passwords \cite{Docker2016Docker}.

  To cope with these disadvantages, the ambassador pattern was introduced \cite{Docker2016Docker}. The idea behind that pattern is the introduction of a container that acts as an intermediator between two services. This container is linked to both original containers and forwards their requests. In case that one of them needs to be restarted, this container is restarted to restore the connection -- in place of the other container.
  By using two ambassador containers that point at each other, a multi-host setup can be created.
  An obvious drawback of this pattern is, that it does not scale well, since each connection requires at least one additional container to be added to the setup. In a clique-like connection setup between five containers \emph{on the same host}, the ambassador pattern would already require ten additional containers. In a two-host setup, this number would grow to twenty additional containers.

  Another solution is based on the Docker networking feature set, which was introduced in the end of 2015.
  This solution utilizes overlay networks, which were briefly presented in \ref{sub:docker_networks}. During the development of this feature set, the \emph{Container Network Model} was added, which allows containers to become member of multiple networks \cite{Tucker2015Docker}.
  This enables the creation of purpose-oriented networks, \eg a ``backend'' and a ``frontend'' network. Containers that are members of both networks can communicate with all containers, while containers which are exclusively connected to the frontend network can only see other containers of this network. This way, one could for example force access to a database to be routed through a container that filters malicious requests. This concept can be adapted for the prototype by adding a third network ``enactment'', which permits both custom and third-party containers that are started in the course of workflow enactments to communicate with some components of the \ac{WfMS}, \eg the \ac{MOM}, while keeping these containers isolated from the rest of the \ac{WfMS}.

  With Docker networking, each service can simply be addressed by its container's name and its corresponding port. As long as their required ports are not exposed on the host's network interface, any number of containers may be reachable on the same port. This is especially beneficial for running multiple containers that offer the same application, since they all can use their default ports.

  As the last approach offers the required capabilities and, at the time of this writing, had no known drawbacks, it should be favored over the other presented ones.
% subsection inter_component_communication (end)
